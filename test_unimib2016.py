#!/usr/bin/env python3
"""
UNIMIB2016 é£Ÿç‰©åˆ†ç±»æµ‹è¯•è„šæœ¬
UNIMIB2016 Food Classification Test Script

å…¨é¢æµ‹è¯•è®­ç»ƒå¥½çš„é£Ÿç‰©åˆ†ç±»æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹éªŒè¯å’Œè¯„ä¼°
- æ€§èƒ½æŒ‡æ ‡è®¡ç®—
- æ··æ·†çŸ©é˜µç”Ÿæˆ
- åˆ†ç±»æŠ¥å‘Š
- é”™è¯¯æ¡ˆä¾‹åˆ†æ
- å¯è§†åŒ–ç»“æœ
"""

import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys
import json
import time
from PIL import Image
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ  YOLOv5 è·¯å¾„
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

from models.common import DetectMultiBackend
from utils.general import check_img_size, increment_path
from utils.torch_utils import select_device
from utils.dataloaders import create_classification_dataloader

# UNIMIB2016 ç±»åˆ«åç§°
UNIMIB2016_CLASSES = {
    0: 'é¢åŒ…', 1: 'ç•ªèŒ„é…±æ„é¢', 2: 'è‚‰é…±æ„é¢', 3: 'è›¤èœŠé…±æ„é¢', 4: 'é’é…±æ„é¢',
    5: 'è’œè“‰æ©„æ¦„æ²¹æ„é¢', 6: 'ç•ªèŒ„é…±åœŸè±†å›¢å­', 7: 'é’é…±åœŸè±†å›¢å­', 8: 'æ„å¼çƒ©é¥­', 9: 'ç‰ç±³ç²¥',
    10: 'ç›æ ¼ä¸½ç‰¹æŠ«è¨', 11: 'å››ç§å¥¶é…ªæŠ«è¨', 12: 'è”¬èœæŠ«è¨', 13: 'ç«è…¿æŠ«è¨', 14: 'ä½›å¡å¤é¢åŒ…',
    15: 'è±Œè±†ç«è…¿åŒ…', 16: 'å®½é¢æ¡é…é…±', 17: 'ç•ªèŒ„é…±è‚‰ä¸¸', 18: 'çƒ¤æ„é¢', 19: 'æ„é¢æ²™æ‹‰',
    20: 'è”¬èœæ±¤', 21: 'é±¼æ±¤', 22: 'è”¬èœæ±¤2', 23: 'ç‰›è‚š', 24: 'æ„é¢è±†æ±¤',
    25: 'æ‰˜æ–¯å¡çº³è”¬èœæ±¤', 26: 'çƒ¤é±¼', 27: 'ä»€é”¦ç‚¸é±¼', 28: 'è£¹ç²‰ç‚¸é±¼', 29: 'çƒ¤é¸¡',
    30: 'é¸¡èƒ¸è‚‰', 31: 'é¸¡ç¿…', 32: 'ç‚¸é¸¡', 33: 'å°ç‰›æ’', 34: 'çƒ¤ç‰›è‚‰',
    35: 'ç‚–ç‰›è‚‰', 36: 'çƒ¤ç‰›è‚‰2', 37: 'æ±‰å ¡', 38: 'çŒªæ’', 39: 'çŒªé‡Œè„Š',
    40: 'çƒ¤çŒªè‚‰', 41: 'ç”Ÿç«è…¿', 42: 'ç†Ÿç«è…¿', 43: 'ç…è›‹', 44: 'ç‚’è›‹',
    45: 'æ°´ç…®è›‹', 46: 'ç…è›‹å·', 47: 'å¥¶é…ª', 48: 'é©¬è‹é‡Œæ‹‰å¥¶é…ª', 49: 'èŒ…å±‹å¥¶é…ª',
    50: 'é…¸å¥¶', 51: 'è‹¹æœ', 52: 'é¦™è•‰', 53: 'æ©™å­', 54: 'è‰è“',
    55: 'è‘¡è„', 56: 'æ¢¨', 57: 'æ¡ƒå­', 58: 'æŸ æª¬', 59: 'çŒ•çŒ´æ¡ƒ',
    60: 'è è', 61: 'æ··åˆæ²™æ‹‰', 62: 'èƒ¡èåœ', 63: 'é’è±†', 64: 'è èœ',
    65: 'ç•ªèŒ„', 66: 'åœŸè±†', 67: 'è–¯æ¡', 68: 'çƒ¤åœŸè±†', 69: 'æ°´ç…®åœŸè±†',
    70: 'åœŸè±†å›¢å­', 71: 'è‘¡è„é…’', 72: 'æ°´'
}

class FoodClassificationTester:
    def __init__(self, weights_path, data_yaml, device=''):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.weights_path = Path(weights_path)
        self.data_yaml = data_yaml
        self.device = select_device(device)
        self.model = None
        self.results = {}
        
        print(f"ğŸ—ï¸ åˆå§‹åŒ–æµ‹è¯•å™¨")
        print(f"ğŸ“‚ æ¨¡å‹æƒé‡: {self.weights_path}")
        print(f"ğŸ“Š æ•°æ®é…ç½®: {self.data_yaml}")
        print(f"ğŸ’» è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = DetectMultiBackend(self.weights_path, device=self.device)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def test_single_image(self, image_path, show_result=True):
        """æµ‹è¯•å•å¼ å›¾åƒ"""
        image_path = Path(image_path)
        if not image_path.exists():
            print(f"âŒ å›¾åƒä¸å­˜åœ¨: {image_path}")
            return None
        
        print(f"\nğŸ–¼ï¸ æµ‹è¯•å›¾åƒ: {image_path.name}")
        
        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
        try:
            img = Image.open(image_path).convert('RGB')
            img_array = np.array(img)
            
            # è½¬æ¢ä¸ºtensor
            img_tensor = torch.from_numpy(img_array).to(self.device)
            img_tensor = img_tensor.permute(2, 0, 1).float() / 255.0
            img_tensor = img_tensor.unsqueeze(0)
            
            # é¢„æµ‹
            start_time = time.time()
            with torch.no_grad():
                pred = self.model(img_tensor)
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # å¤„ç†é¢„æµ‹ç»“æœ
            if hasattr(pred, 'softmax'):
                probs = pred.softmax(1)
            else:
                probs = torch.softmax(pred, dim=1)
            
            # è·å–top5é¢„æµ‹
            top5_probs, top5_indices = torch.topk(probs[0], 5)
            
            results = []
            for i, (prob, idx) in enumerate(zip(top5_probs, top5_indices)):
                class_id = idx.item()
                confidence = prob.item()
                class_name = UNIMIB2016_CLASSES.get(class_id, f'æœªçŸ¥ç±»åˆ«_{class_id}')
                results.append({
                    'rank': i + 1,
                    'class_id': class_id,
                    'class_name': class_name,
                    'confidence': confidence
                })
            
            # æ˜¾ç¤ºç»“æœ
            if show_result:
                print(f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.2f}ms")
                print("ğŸ“Š Top5 é¢„æµ‹ç»“æœ:")
                print("-" * 50)
                for result in results:
                    print(f"ç¬¬{result['rank']}å: {result['class_name']} "
                          f"(ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            
            return {
                'image_path': str(image_path),
                'inference_time_ms': inference_time,
                'predictions': results
            }
            
        except Exception as e:
            print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
            return None
    
    def test_dataset(self, data_yaml, batch_size=32, save_results=True):
        """æµ‹è¯•æ•´ä¸ªæ•°æ®é›†"""
        print(f"\nğŸ“Š å¼€å§‹æ•°æ®é›†è¯„ä¼°")
        print(f"ğŸ“‚ æ•°æ®é…ç½®: {data_yaml}")
        print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        try:
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = create_classification_dataloader(
                path=data_yaml,
                imgsz=224,
                batch_size=batch_size,
                augment=False,
                cache=False,
                rank=-1,
                workers=4,
                shuffle=False
            )[0]
            
            print(f"ğŸ“ˆ æµ‹è¯•é›†å¤§å°: {len(dataloader.dataset)} å¼ å›¾åƒ")
            
            # è¯„ä¼°æ¨¡å‹
            self.model.eval()
            all_predictions = []
            all_targets = []
            inference_times = []
            
            print("ğŸ”„ å¼€å§‹è¯„ä¼°...")
            for batch_i, (images, targets) in enumerate(dataloader):
                images = images.to(self.device)
                targets = targets.to(self.device)
                
                # æ¨ç†
                start_time = time.time()
                with torch.no_grad():
                    pred = self.model(images)
                inference_time = (time.time() - start_time) * 1000
                inference_times.append(inference_time)
                
                # è·å–é¢„æµ‹ç±»åˆ«
                if hasattr(pred, 'softmax'):
                    probs = pred.softmax(1)
                else:
                    probs = torch.softmax(pred, dim=1)
                
                predicted_classes = torch.argmax(probs, dim=1)
                
                all_predictions.extend(predicted_classes.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                
                if (batch_i + 1) % 10 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {batch_i + 1}/{len(dataloader)} æ‰¹æ¬¡")
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(all_targets, all_predictions)
            precision, recall, f1, support = precision_recall_fscore_support(
                all_targets, all_predictions, average='weighted'
            )
            
            avg_inference_time = np.mean(inference_times)
            
            # ä¿å­˜ç»“æœ
            self.results = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'avg_inference_time_ms': avg_inference_time,
                'total_samples': len(all_targets),
                'predictions': all_predictions,
                'targets': all_targets
            }
            
            # æ‰“å°ç»“æœ
            print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
            print(f"âœ… å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
            print(f"ğŸ¯ ç²¾ç¡®ç‡: {precision:.4f}")
            print(f"ğŸ” å¬å›ç‡: {recall:.4f}")
            print(f"ğŸ† F1åˆ†æ•°: {f1:.4f}")
            print(f"â±ï¸ å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.2f}ms/æ‰¹æ¬¡")
            
            if save_results:
                self.save_detailed_results()
                self.generate_confusion_matrix()
                self.generate_classification_report()
                self.analyze_errors()
            
            return self.results
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def save_detailed_results(self):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        save_dir = Path('runs/test') / 'unimib2016_results'
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ•°å€¼ç»“æœ
        results_file = save_dir / 'test_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            results_copy = self.results.copy()
            results_copy['predictions'] = [int(x) for x in self.results['predictions']]
            results_copy['targets'] = [int(x) for x in self.results['targets']]
            json.dump(results_copy, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def generate_confusion_matrix(self):
        """ç”Ÿæˆæ··æ·†çŸ©é˜µ"""
        save_dir = Path('runs/test') / 'unimib2016_results'
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.results['targets'], self.results['predictions'])
        
        # åˆ›å»ºå¯è§†åŒ–
        plt.figure(figsize=(20, 16))
        
        # ç”±äºç±»åˆ«å¤ªå¤šï¼Œåªæ˜¾ç¤ºç±»åˆ«ID
        class_ids = list(range(len(UNIMIB2016_CLASSES)))
        
        sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',
                   xticklabels=class_ids, yticklabels=class_ids)
        plt.title('UNIMIB2016 é£Ÿç‰©åˆ†ç±»æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
        plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
        plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
        plt.xticks(rotation=90)
        plt.yticks(rotation=0)
        
        # ä¿å­˜å›¾åƒ
        cm_file = save_dir / 'confusion_matrix.png'
        plt.tight_layout()
        plt.savefig(cm_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {cm_file}")
        
        # ä¿å­˜æ··æ·†çŸ©é˜µæ•°æ®
        cm_data_file = save_dir / 'confusion_matrix.csv'
        cm_df = pd.DataFrame(cm, index=class_ids, columns=class_ids)
        cm_df.to_csv(cm_data_file)
        
        return cm
    
    def generate_classification_report(self):
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
        save_dir = Path('runs/test') / 'unimib2016_results'
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        class_names = [UNIMIB2016_CLASSES[i] for i in range(len(UNIMIB2016_CLASSES))]
        report = classification_report(
            self.results['targets'], 
            self.results['predictions'],
            target_names=class_names,
            output_dict=True
        )
        
        # ä¿å­˜ä¸ºCSV
        report_df = pd.DataFrame(report).transpose()
        report_file = save_dir / 'classification_report.csv'
        report_df.to_csv(report_file, encoding='utf-8-sig')
        
        # ä¿å­˜ä¸ºæ–‡æœ¬
        text_report = classification_report(
            self.results['targets'], 
            self.results['predictions'],
            target_names=class_names
        )
        
        report_text_file = save_dir / 'classification_report.txt'
        with open(report_text_file, 'w', encoding='utf-8') as f:
            f.write("UNIMIB2016 é£Ÿç‰©åˆ†ç±»è¯¦ç»†æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(text_report)
        
        print(f"ğŸ“‹ åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print(f"ğŸ“„ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_text_file}")
    
    def analyze_errors(self, top_errors=10):
        """åˆ†æé”™è¯¯æ¡ˆä¾‹"""
        save_dir = Path('runs/test') / 'unimib2016_results'
        
        predictions = np.array(self.results['predictions'])
        targets = np.array(self.results['targets'])
        
        # æ‰¾å‡ºé”™è¯¯é¢„æµ‹
        errors = predictions != targets
        error_indices = np.where(errors)[0]
        
        print(f"\nğŸ” é”™è¯¯åˆ†æ:")
        print(f"âŒ é”™è¯¯é¢„æµ‹æ•°é‡: {len(error_indices)}")
        print(f"âœ… æ­£ç¡®é¢„æµ‹æ•°é‡: {len(targets) - len(error_indices)}")
        print(f"ğŸ“Š é”™è¯¯ç‡: {len(error_indices)/len(targets)*100:.2f}%")
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„é”™è¯¯
        error_stats = {}
        for idx in error_indices:
            true_class = targets[idx]
            pred_class = predictions[idx]
            
            if true_class not in error_stats:
                error_stats[true_class] = {'total_errors': 0, 'confused_with': {}}
            
            error_stats[true_class]['total_errors'] += 1
            
            if pred_class not in error_stats[true_class]['confused_with']:
                error_stats[true_class]['confused_with'][pred_class] = 0
            error_stats[true_class]['confused_with'][pred_class] += 1
        
        # ä¿å­˜é”™è¯¯åˆ†æ
        error_analysis = {}
        for class_id, stats in error_stats.items():
            class_name = UNIMIB2016_CLASSES[class_id]
            confused_with = []
            for confused_class_id, count in stats['confused_with'].items():
                confused_class_name = UNIMIB2016_CLASSES[confused_class_id]
                confused_with.append({
                    'class_id': confused_class_id,
                    'class_name': confused_class_name,
                    'count': count
                })
            
            error_analysis[str(class_id)] = {
                'class_name': class_name,
                'total_errors': stats['total_errors'],
                'confused_with': sorted(confused_with, key=lambda x: x['count'], reverse=True)
            }
        
        error_file = save_dir / 'error_analysis.json'
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(error_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ” é”™è¯¯åˆ†æå·²ä¿å­˜åˆ°: {error_file}")
        
        # æ˜¾ç¤ºæœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«
        print(f"\nğŸ¤” æœ€å®¹æ˜“å‡ºé”™çš„å‰ {top_errors} ä¸ªç±»åˆ«:")
        print("-" * 60)
        sorted_errors = sorted(error_stats.items(), 
                              key=lambda x: x[1]['total_errors'], reverse=True)
        
        for i, (class_id, stats) in enumerate(sorted_errors[:top_errors]):
            class_name = UNIMIB2016_CLASSES[class_id]
            print(f"{i+1:2d}. {class_name} (ID: {class_id}) - {stats['total_errors']} ä¸ªé”™è¯¯")
            
            # æ˜¾ç¤ºæœ€å¸¸æ··æ·†çš„ç±»åˆ«
            sorted_confused = sorted(stats['confused_with'].items(), 
                                   key=lambda x: x[1], reverse=True)
            for confused_id, count in sorted_confused[:3]:
                confused_name = UNIMIB2016_CLASSES[confused_id]
                print(f"     â†³ å¸¸è¢«è¯¯è®¤ä¸º: {confused_name} ({count} æ¬¡)")

def main():
    parser = argparse.ArgumentParser(description='UNIMIB2016 é£Ÿç‰©åˆ†ç±»æµ‹è¯•è„šæœ¬')
    parser.add_argument('--weights', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data', type=str, default='data/unimib2016.yaml',
                       help='æ•°æ®é›†é…ç½®æ–‡ä»¶')
    parser.add_argument('--device', default='',
                       help='æ¨ç†è®¾å¤‡ (cpu, 0, 1, 2, 3, ...)')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--single-image', type=str,
                       help='æµ‹è¯•å•å¼ å›¾åƒè·¯å¾„')
    parser.add_argument('--test-dataset', action='store_true',
                       help='æµ‹è¯•æ•´ä¸ªæ•°æ®é›†')
    parser.add_argument('--save-results', action='store_true', default=True,
                       help='ä¿å­˜æµ‹è¯•ç»“æœ')
    
    args = parser.parse_args()
    
    print("ğŸ• UNIMIB2016 é£Ÿç‰©åˆ†ç±»æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = FoodClassificationTester(args.weights, args.data, args.device)
    
    # æµ‹è¯•å•å¼ å›¾åƒ
    if args.single_image:
        result = tester.test_single_image(args.single_image)
        if result:
            print(f"\nâœ… å•å›¾åƒæµ‹è¯•å®Œæˆ")
    
    # æµ‹è¯•æ•°æ®é›†
    if args.test_dataset:
        results = tester.test_dataset(args.data, args.batch_size, args.save_results)
        if results:
            print(f"\nâœ… æ•°æ®é›†æµ‹è¯•å®Œæˆ")
            print(f"ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡: {results['accuracy']*100:.2f}%")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    if args.save_results:
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: runs/test/unimib2016_results/")

if __name__ == '__main__':
    main()
